# HanBert-Transformers

[HanBert](https://github.com/tbai2019/HanBert-54k-N) on ğŸ¤— Huggingface Transformers ğŸ¤—

## Details

- HanBert Tensorflow ckptë¥¼ Pytorchë¡œ ë³€í™˜
  - ê¸°ì¡´ì˜ Optimizer ê´€ë ¨ ParameterëŠ” ì œê±°í•˜ì—¬ ê¸°ì¡´ì˜ 1.43GBì—ì„œ 488MBë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.
  - **ë³€í™˜ ì‹œ Optimizer ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ Skip í•˜ì§€ ëª»í•˜ëŠ” ì´ìŠˆê°€ ìˆì–´ í•´ë‹¹ ë¶€ë¶„ì„ ê³ ì³ì„œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.** ([í•´ë‹¹ ì´ìŠˆ ê´€ë ¨ PR](https://github.com/huggingface/transformers/pull/2652))

```bash
# transformers bert TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT
$ transformers bert HanBert-54kN/model.ckpt-3000000 \
                    HanBert-54kN/bert_config.json \
                    HanBert-54kN/pytorch_model.bin
```

- Tokenizerë¥¼ ìœ„í•˜ì—¬ `tokenization_hanbert.py` íŒŒì¼ì„ ìƒˆë¡œ ì œì‘
  - Transformersì˜ **tokenization ê´€ë ¨ í•¨ìˆ˜ ì§€ì›** (`convert_tokens_to_ids`, `convert_tokens_to_string`, `encode_plus`...)

## How to Use

1. **ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**

   - torch>=1.1.0
   - transformers>=2.2.2

2. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶• í•´ì œ**

   - ê¸°ì¡´ì˜ HanBertì—ì„œëŠ” tokenization ê´€ë ¨ íŒŒì¼ì„ `/usr/local/moran`ì— ë³µì‚¬í•´ì•¼ í–ˆì§€ë§Œ, í•´ë‹¹ í´ë” ì´ìš© ì‹œ ê·¸ë˜ë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
   - **ë‹¤ìš´ë¡œë“œ ë§í¬ (Pretrained weight + Tokenizer tool)**
     - [HanBert-54kN-torch](https://drive.google.com/open?id=1LUyrnhuNC3e8oD2QMJv8tIDrXrxzmdu4)
     - [HanBert-54kN-IP-torch](https://drive.google.com/open?id=1wjROsuDKoJQx4Pu0nqSefVDs3echKSXP)

3. **tokenization_hanbert.py ì¤€ë¹„**

   - Tokenizerì˜ ê²½ìš° **Ubuntu** í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
   - í•˜ë‹¨ì˜ í˜•íƒœë¡œ ë””ë ‰í† ë¦¬ê°€ ì„¸íŒ…ì´ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

```
.
â”œâ”€â”€ ...
â”œâ”€â”€ HanBert-54kN-torch
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ vocab_54k.txt
â”‚   â”œâ”€â”€ libmoran4dnlp.so
â”‚   â”œâ”€â”€ moran.db
â”‚   â”œâ”€â”€ udict.txt
â”‚   â””â”€â”€ uentity.txt
â”œâ”€â”€ tokenization_hanbert.py
â””â”€â”€ ...
```

## Example

### 1. Model

```python
>>> import torch
>>> from transformers import BertModel

>>> model = BertModel.from_pretrained('HanBert-54kN-torch')
>>> input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])
>>> token_type_ids = torch.LongTensor([[0, 0, 0], [0, 0, 0]])
>>> attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])
>>> sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)
>>> sequence_output
tensor([[[-0.0938, -0.5030,  0.3765,  ..., -0.4880, -0.4486,  0.3600],
         [-0.6036, -0.1008, -0.2344,  ..., -0.6606, -0.5762,  0.1021],
         [-0.4353,  0.0970, -0.0781,  ..., -0.7686, -0.4418,  0.4109]],

        [[-0.7117,  0.2479, -0.8164,  ...,  0.1509,  0.8337,  0.4054],
         [-0.7867, -0.0443, -0.8754,  ...,  0.0952,  0.5044,  0.5125],
         [-0.8613,  0.0138, -0.9315,  ...,  0.1651,  0.6647,  0.5509]]],
       grad_fn=<AddcmulBackward>)
```

### 2. Tokenizer

```python
>>> from tokenization_hanbert import HanBertTokenizer
>>> tokenizer = HanBertTokenizer.from_pretrained('HanBert-54kN-torch')
>>> text = "ë‚˜ëŠ” ê±¸ì–´ê°€ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤. ë‚˜ëŠ”ê±¸ì–´ ê°€ê³ ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤. ì˜ ë¶„ë¥˜ë˜ê¸°ë„ í•œë‹¤. ì˜ ë¨¹ê¸°ë„ í•œë‹¤."
>>> tokenizer.tokenize(text)
['ë‚˜', '~~ëŠ”', 'ê±¸ì–´ê°€', '~~ê³ ', 'ìˆ', '~~ëŠ”', 'ì¤‘', '~~ì…', '~~ë‹ˆë‹¤', '.', 'ë‚˜', '##ëŠ”ê±¸', '##ì–´', 'ê°€', '~~ê³ ', '~ìˆ', '~~ëŠ”', 'ì¤‘', '~~ì…', '~~ë‹ˆë‹¤', '.', 'ì˜', 'ë¶„ë¥˜', '~~ë˜', '~~ê¸°', '~~ë„', 'í•œ', '~~ë‹¤', '.', 'ì˜', 'ë¨¹', '~~ê¸°', '~~ë„', 'í•œ', '~~ë‹¤', '.']
```

### 3. Test with python file

```bash
$ python3 test_hanbert.py
$ python3 test_hanbert_ip.py
```

## Result on Subtask

`max_seq_len = 50`ìœ¼ë¡œ ì„¤ì •

|                   | **NSMC** (acc) | **Naver-NER** (F1) |
| ----------------- | -------------- | ------------------ |
| HanBert-54kN      | **90.16**      | **84.84**          |
| HanBert-54kN-IP   | 88.72          | 84.45              |
| KoBERT            | 89.63          | 84.23              |
| Bert-multilingual | 87.07          | 81.78              |

- NSMC (Naver Sentiment Movie Corpus) ([Implementation of HanBert-nsmc](https://github.com/monologg/HanBert-nsmc))
- Naver NER (NER task on Naver NLP Challenge 2018) ([Implementation of HanBert-NER](https://github.com/monologg/HanBert-NER))

## Reference

- [HanBert Github](https://github.com/tbai2019/HanBert-54k-N)
- [KoBERT Github](https://github.com/SKTBrain/KoBERT)
- [Kobert-transformers](https://pypi.org/project/kobert-transformers/)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [From Tensorflow to Pytorch](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)
